{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1deb7e63-0f2a-400b-a469-3c00e0155fa8",
   "metadata": {},
   "source": [
    "# Facenet recreation\n",
    "Google facenet recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab0145-5c39-48a4-a6b3-65ecf44edead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "from matplotlib import image\n",
    "import glob as glob\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  print(\"Using the GPU!\")\n",
    "else:\n",
    "  print(\"WARNING: Could not find GPU! Using CPU only. If you want to enable GPU, please to go Edit > Notebook Settings > Hardware Accelerator and select GPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c3927d-59f0-471c-9ec0-91d4ad88a695",
   "metadata": {},
   "source": [
    "# Build Dataset from facenet\n",
    "using instructions from pytorch docs: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "Reading pairs to ensure that muiltiple faces of a single person \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27336115-5844-4787-9294-8fe5afde62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabledFacesWild(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        label_dict = []\n",
    "        with open(annotations_file) as f:\n",
    "            for line in f:\n",
    "                name, i1, i2 = line.split('\\t')\n",
    "                label_dict.append({\n",
    "                    \"label\": name,\n",
    "                    \"img1\": f\"{name}/{i1:04d}.jpg\",\n",
    "                    \"img2\": f\"{name}/{i2:04d}.jpg\"\n",
    "                })\n",
    "        self.img_labels = pd.DataFrame.from_records(label_dict)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels) * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair_num = idx // 2\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[pair_num, (idx % 2) + 1])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[pair_num, 0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee94ce5-1029-4fe2-b740-3f88501f8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually load data\n",
    "num_train_pairs = 1100\n",
    "num_test_pairs = 500\n",
    "batch_size = 40\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# construct the dataloader\n",
    "train_data = Edges2Image('./pairsDevTrain.txt', './lfw/', transform)\n",
    "test_data = Edges2Image('./pairsDevTest.txt', './lfw/', transform)\n",
    "train_loader = DataLoader(train_data, batch_size=50, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=50, shuffle=False)\n",
    "\n",
    "# todo figure out actual batching while including groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d190bd8c-ffa6-4bae-8d13-5de9a4dac358",
   "metadata": {},
   "source": [
    "# Create Facenet Model\n",
    "Facenet model https://arxiv.org/pdf/1503.03832.pdf\n",
    "Google Lenet 22layer: https://arxiv.org/pdf/1409.4842.pdf\n",
    "input size = 250x250x3\n",
    "use relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b9419-5379-4000-b1de-b2ea3ef7a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Facenet_NN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "        # torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75, k=1.0)\n",
    "        self.conv1 = nn.Conv2d(3, 64, 7, stride=2)\n",
    "        self.pool1 = nn.MaxPool2d(3, stride=2)\n",
    "        self.rnorm1 = nn.LocalResponseNorm(64)\n",
    "        \n",
    "        self.conv2a = nn.Conv2d(64, 64, 1, stride=1)\n",
    "        self.conv2 = nn.Conv2d(64, 192, 3, stride=1)\n",
    "        self.rnorm2 = nn.LocalResponseNorm(192)\n",
    "        self.pool2 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "        self.conv3a = nn.Conv2d(192, 192, 1, stride=1)\n",
    "        self.conv3 = nn.Conv2d(192, 384, 3, stride=1)\n",
    "        self.pool3 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "        self.conv4a = nn.Conv2d(384, 384, 1, stride=1)\n",
    "        self.conv4 = nn.Conv2d(384, 256, 3, stride=1)\n",
    "\n",
    "        self.conv5a = nn.Conv2d(256, 256, 1, stride=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, 3, stride=1)\n",
    "\n",
    "        self.conv6a = nn.Conv2d(256, 256, 1, stride=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, 3, stride=1)\n",
    "        self.pool4 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "        #todo what is concat layer?\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.maxout1 = nn.MaxPool2d(2)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.maxout2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.fc7128 = nn.Linear(128,128)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.rnorm1(self.pool1(F.relu(self.conv1(x))))\n",
    "        x2 = self.pool2(self.rnorm2(F.relu(self.conv2(self.conv2a(x1)))))\n",
    "        x3 = self.pool3(F.relu(self.conv3(self.conv3a(x2))))\n",
    "        x4 = F.relu(self.conv4(self.conv4a(x3)))\n",
    "        x5 = F.relu(self.conv5(self.conv5a(x4)))\n",
    "        x6 = self.pool4(F.relu(self.conv6(self.conv6a)))\n",
    "        x7 = self.maxout2(self.fc2(self.maxout1(self.fc1(x6))))\n",
    "        return self.fc7128(x7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fcaa6b-902b-4f5c-b48b-2ec1aa46ee89",
   "metadata": {},
   "source": [
    "# Train Facenet Model\n",
    "training data\n",
    "In all our experiments we train the CNN using Stochastic\n",
    "Gradient Descent (SGD) with standard backprop [8, 11] and\n",
    "AdaGrad [5]. In most experiments we start with a learning\n",
    "rate of 0.05 which we lower to finalize the model. The models are initialized from random, similar to [16], and trained\n",
    "on a CPU cluster for 1,000 to 2,000 hours. The decrease in\n",
    "the loss (and increase in accuracy) slows down drastically\n",
    "after 500h of training, but additional training can still significantly improve performance. The margin Î± is set to 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df9202-813b-4d3e-8b5a-d4f5e6167eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc018e-1f6d-4840-ab6f-f2bb505cbdc0",
   "metadata": {},
   "source": [
    "# Test Facenet Model\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694dafc5-06c2-486c-997b-b4f97ac4f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
