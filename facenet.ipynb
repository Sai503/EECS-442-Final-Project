{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1deb7e63-0f2a-400b-a469-3c00e0155fa8",
   "metadata": {},
   "source": [
    "# Facenet recreation\n",
    "Google facenet recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9ab0145-5c39-48a4-a6b3-65ecf44edead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.1.1\n",
      "Torchvision Version:  0.16.1\n",
      "Using the GPU!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "from matplotlib import image\n",
    "import glob as glob\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "  print(\"Using the GPU!\")\n",
    "else:\n",
    "  print(\"WARNING: Could not find GPU! Using CPU only. If you want to enable GPU, please to go Edit > Notebook Settings > Hardware Accelerator and select GPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c3927d-59f0-471c-9ec0-91d4ad88a695",
   "metadata": {},
   "source": [
    "# Build Dataset from facenet\n",
    "using instructions from pytorch docs: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "Reading pairs to ensure that muiltiple faces of a single person \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27336115-5844-4787-9294-8fe5afde62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LabledFacesWild(Dataset):\n",
    "#     def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "#         label_dict = []\n",
    "#         with open(annotations_file) as f:\n",
    "#             for line in f:\n",
    "#                 print(line)\n",
    "#                 name, i1, i2 = line.split('\\t')\n",
    "#                 label_dict.append({\n",
    "#                     \"label\": name,\n",
    "#                     \"img1\": f\"{name}/{int(i1):04d}.jpg\",\n",
    "#                     \"img2\": f\"{name}/{int(i2):04d}.jpg\"\n",
    "#                 })\n",
    "#         self.img_labels = pd.DataFrame.from_records(label_dict)\n",
    "#         self.img_dir = img_dir\n",
    "#         self.transform = transform\n",
    "#         self.target_transform = target_transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.img_labels) * 2\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         pair_num = idx // 2\n",
    "#         img_path = os.path.join(self.img_dir, self.img_labels.iloc[pair_num, (idx % 2) + 1])\n",
    "#         image = read_image(img_path)\n",
    "#         label = self.img_labels.iloc[pair_num, 0]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         if self.target_transform:\n",
    "#             label = self.target_transform(label)\n",
    "#         return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSFT_Faces(Dataset):\n",
    "    def __init__(self, img_dir, start_idx, end_idx, img_per_id, transform=None):\n",
    "        self.start_idx = start_idx\n",
    "        self.end_idx = end_idx\n",
    "        self.img_per_id = img_per_id\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end_idx - self.start_idx + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        person = idx + self.start_idx\n",
    "        images = []\n",
    "        for i in range(self.img_per_id):\n",
    "            img_path = os.path.join(self.img_dir, f\"{person}/{i}.png\")\n",
    "            image = torchvision.io.read_image(img_path)\n",
    "            # remove alpha channel\n",
    "            image = image[:3, :, :]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            images.append(image)\n",
    "        # create array of labels\n",
    "        label = torch.tensor(np.full((self.img_per_id,), person))\n",
    "        return torch.tensor(images), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ee94ce5-1029-4fe2-b740-3f88501f8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually load data\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# construct the dataloader\n",
    "train_data = MSFT_Faces('./MSFT_1M_FACES/', 0, 3999, 40, transform)\n",
    "test_data = MSFT_Faces('./MSFT_1M_FACES/', 8000, 9999, 40, transform)\n",
    "train_loader = DataLoader(train_data, batch_size=20, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=20, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d190bd8c-ffa6-4bae-8d13-5de9a4dac358",
   "metadata": {},
   "source": [
    "# Create Facenet Model\n",
    "Facenet model https://arxiv.org/pdf/1503.03832.pdf\n",
    "Google Lenet 22layer: https://arxiv.org/pdf/1409.4842.pdf\n",
    "input size = 250x250x3\n",
    "use relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "261b9419-5379-4000-b1de-b2ea3ef7a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Facenet_NN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "        # torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75, k=1.0)\n",
    "        self.conv1 = nn.Conv2d(3, 64, 7,padding=3, stride=2)\n",
    "        self.pool1 = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "        self.rnorm1 = nn.LocalResponseNorm(64)\n",
    "        \n",
    "        self.conv2a = nn.Conv2d(64, 64, 1, stride=1)\n",
    "        self.conv2 = nn.Conv2d(64, 192, 3,padding=1, stride=1)\n",
    "        self.rnorm2 = nn.LocalResponseNorm(192)\n",
    "        self.pool2 = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "\n",
    "        self.conv3a = nn.Conv2d(192, 192, 1, stride=1)\n",
    "        self.conv3 = nn.Conv2d(192, 384, 3, padding=1, stride=1)\n",
    "        self.pool3 = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "\n",
    "        self.conv4a = nn.Conv2d(384, 384, 1, stride=1)\n",
    "        self.conv4 = nn.Conv2d(384, 256, 3, padding=1, stride=1)\n",
    "\n",
    "        self.conv5a = nn.Conv2d(256, 256, 1, stride=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, 3,padding=1, stride=1)\n",
    "\n",
    "        self.conv6a = nn.Conv2d(256, 256, 1, stride=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, 3,padding=1, stride=1)\n",
    "        self.pool4 = nn.MaxPool2d(3,padding=1, stride=2)\n",
    "\n",
    "        #todo what is concat layer?\n",
    "        self.fc1 = nn.Linear(256*4*4, 128*4*4)\n",
    "        # self.maxout1 = nn.AdaptiveMaxPool2d((32,1))\n",
    "        self.fc2 = nn.Linear(128 * 4 *4, 128 * 4)\n",
    "        # self.maxout2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.fc7128 = nn.Linear(128*4,128)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x1 = self.rnorm1(self.pool1(F.relu(self.conv1(x))))\n",
    "        x2 = self.pool2(self.rnorm2(F.relu(self.conv2(self.conv2a(x1)))))\n",
    "        x3 = self.pool3(F.relu(self.conv3(self.conv3a(x2))))\n",
    "        x4 = F.relu(self.conv4(self.conv4a(x3)))\n",
    "        x5 = F.relu(self.conv5(self.conv5a(x4)))\n",
    "        x6 = self.pool4(F.relu(self.conv6(self.conv6a(x5))))\n",
    "        x6i = torch.flatten(x6, 1)\n",
    "        x7 = self.fc1(x6i)\n",
    "        # x7i = torch.unflatten(x7, 1, (128, 7, 7))\n",
    "        # x8 = self.maxout1(x7i)\n",
    "        # x8i = torch.flatten(x8, 1)\n",
    "        x9 = self.fc2(x7)\n",
    "        # x9i = torch.unflatten(x9, 1, (128, 32))\n",
    "        # x10 = self.maxout2(x9i)\n",
    "        # x10i = torch.flatten(x10, 1)\n",
    "        x11 = self.fc7128(x9)\n",
    "        # normalize the output to a unit vector\n",
    "        x11 = F.normalize(x11)\n",
    "\n",
    "        #x7 = self.maxout2(self.fc2(self.maxout1(self.fc1(x6))))\n",
    "        # ─Conv2d: 1-16                           [1, 256, 6, 6]            590,080\n",
    "        # ─MaxPool2d: 1-17                        [1, 256, 2, 2]            --\n",
    "        # return self.fc1(x6)\n",
    "        return x11\n",
    "        # return self.fc7128(x7)\n",
    "        # return x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create inception module\n",
    "# # https://www.kaggle.com/code/mohamedmustafa/10-implement-inceptionnet-from-scratch-pytorch\n",
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, In_Channels, Out_Channels, Kernel_Size, Stride, Padding):\n",
    "#         super(ConvBlock, self).__init__()\n",
    "#         self.Conv = nn.Conv2d(in_channels=In_Channels, out_channels=Out_Channels, kernel_size=Kernel_Size, stride=Stride, padding=Padding)\n",
    "#         self.Batch_Norm = nn.BatchNorm2d(num_features=Out_Channels)\n",
    "#         self.Activ_Func = nn.ReLU()\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Now we'll build the forward function which defines the path to input tensor\n",
    "#     meaning that we tell the tensor the sequence of layers you're going through \n",
    "#     Takecare the name of forward function is sensitive so you have to name forward not any thing else\n",
    "#     \"\"\"\n",
    "#     def forward(self, Tensor_Path):\n",
    "#         Tensor_Path = self.Conv(Tensor_Path)\n",
    "#         Tensor_Path = self.Batch_Norm(Tensor_Path)\n",
    "#         Tensor_Path = self.Activ_Func(Tensor_Path)\n",
    "        \n",
    "#         return Tensor_Path\n",
    "\n",
    "# class InceptionModule(nn.Module):\n",
    "#     def __init__(self, num1x1, num3x3reduce, num3x3, num5x5reduce, num5x5, ) -> None:\n",
    "#         super().__init__()\n",
    "\n",
    "\n",
    "# # NN2\n",
    "# class Facenet_NN2(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Facenet_NN1                              [10, 128]                 --\n",
      "├─Conv2d: 1-1                            [10, 64, 56, 56]          9,472\n",
      "│    └─weight                                                      ├─9,408\n",
      "│    └─bias                                                        └─64\n",
      "├─MaxPool2d: 1-2                         [10, 64, 28, 28]          --\n",
      "├─LocalResponseNorm: 1-3                 [10, 64, 28, 28]          --\n",
      "├─Conv2d: 1-4                            [10, 64, 28, 28]          4,160\n",
      "│    └─weight                                                      ├─4,096\n",
      "│    └─bias                                                        └─64\n",
      "├─Conv2d: 1-5                            [10, 192, 28, 28]         110,784\n",
      "│    └─weight                                                      ├─110,592\n",
      "│    └─bias                                                        └─192\n",
      "├─LocalResponseNorm: 1-6                 [10, 192, 28, 28]         --\n",
      "├─MaxPool2d: 1-7                         [10, 192, 14, 14]         --\n",
      "├─Conv2d: 1-8                            [10, 192, 14, 14]         37,056\n",
      "│    └─weight                                                      ├─36,864\n",
      "│    └─bias                                                        └─192\n",
      "├─Conv2d: 1-9                            [10, 384, 14, 14]         663,936\n",
      "│    └─weight                                                      ├─663,552\n",
      "│    └─bias                                                        └─384\n",
      "├─MaxPool2d: 1-10                        [10, 384, 7, 7]           --\n",
      "├─Conv2d: 1-11                           [10, 384, 7, 7]           147,840\n",
      "│    └─weight                                                      ├─147,456\n",
      "│    └─bias                                                        └─384\n",
      "├─Conv2d: 1-12                           [10, 256, 7, 7]           884,992\n",
      "│    └─weight                                                      ├─884,736\n",
      "│    └─bias                                                        └─256\n",
      "├─Conv2d: 1-13                           [10, 256, 7, 7]           65,792\n",
      "│    └─weight                                                      ├─65,536\n",
      "│    └─bias                                                        └─256\n",
      "├─Conv2d: 1-14                           [10, 256, 7, 7]           590,080\n",
      "│    └─weight                                                      ├─589,824\n",
      "│    └─bias                                                        └─256\n",
      "├─Conv2d: 1-15                           [10, 256, 7, 7]           65,792\n",
      "│    └─weight                                                      ├─65,536\n",
      "│    └─bias                                                        └─256\n",
      "├─Conv2d: 1-16                           [10, 256, 7, 7]           590,080\n",
      "│    └─weight                                                      ├─589,824\n",
      "│    └─bias                                                        └─256\n",
      "├─MaxPool2d: 1-17                        [10, 256, 4, 4]           --\n",
      "├─Linear: 1-18                           [10, 2048]                8,390,656\n",
      "│    └─weight                                                      ├─8,388,608\n",
      "│    └─bias                                                        └─2,048\n",
      "├─Linear: 1-19                           [10, 512]                 1,049,088\n",
      "│    └─weight                                                      ├─1,048,576\n",
      "│    └─bias                                                        └─512\n",
      "├─Linear: 1-20                           [10, 128]                 65,664\n",
      "│    └─weight                                                      ├─65,536\n",
      "│    └─bias                                                        └─128\n",
      "==========================================================================================\n",
      "Total params: 12,675,392\n",
      "Trainable params: 12,675,392\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 3.82\n",
      "==========================================================================================\n",
      "Input size (MB): 1.51\n",
      "Forward/backward pass size (MB): 47.88\n",
      "Params size (MB): 50.70\n",
      "Estimated Total Size (MB): 100.09\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Facenet_NN1                              [10, 128]                 --\n",
       "├─Conv2d: 1-1                            [10, 64, 56, 56]          9,472\n",
       "│    └─weight                                                      ├─9,408\n",
       "│    └─bias                                                        └─64\n",
       "├─MaxPool2d: 1-2                         [10, 64, 28, 28]          --\n",
       "├─LocalResponseNorm: 1-3                 [10, 64, 28, 28]          --\n",
       "├─Conv2d: 1-4                            [10, 64, 28, 28]          4,160\n",
       "│    └─weight                                                      ├─4,096\n",
       "│    └─bias                                                        └─64\n",
       "├─Conv2d: 1-5                            [10, 192, 28, 28]         110,784\n",
       "│    └─weight                                                      ├─110,592\n",
       "│    └─bias                                                        └─192\n",
       "├─LocalResponseNorm: 1-6                 [10, 192, 28, 28]         --\n",
       "├─MaxPool2d: 1-7                         [10, 192, 14, 14]         --\n",
       "├─Conv2d: 1-8                            [10, 192, 14, 14]         37,056\n",
       "│    └─weight                                                      ├─36,864\n",
       "│    └─bias                                                        └─192\n",
       "├─Conv2d: 1-9                            [10, 384, 14, 14]         663,936\n",
       "│    └─weight                                                      ├─663,552\n",
       "│    └─bias                                                        └─384\n",
       "├─MaxPool2d: 1-10                        [10, 384, 7, 7]           --\n",
       "├─Conv2d: 1-11                           [10, 384, 7, 7]           147,840\n",
       "│    └─weight                                                      ├─147,456\n",
       "│    └─bias                                                        └─384\n",
       "├─Conv2d: 1-12                           [10, 256, 7, 7]           884,992\n",
       "│    └─weight                                                      ├─884,736\n",
       "│    └─bias                                                        └─256\n",
       "├─Conv2d: 1-13                           [10, 256, 7, 7]           65,792\n",
       "│    └─weight                                                      ├─65,536\n",
       "│    └─bias                                                        └─256\n",
       "├─Conv2d: 1-14                           [10, 256, 7, 7]           590,080\n",
       "│    └─weight                                                      ├─589,824\n",
       "│    └─bias                                                        └─256\n",
       "├─Conv2d: 1-15                           [10, 256, 7, 7]           65,792\n",
       "│    └─weight                                                      ├─65,536\n",
       "│    └─bias                                                        └─256\n",
       "├─Conv2d: 1-16                           [10, 256, 7, 7]           590,080\n",
       "│    └─weight                                                      ├─589,824\n",
       "│    └─bias                                                        └─256\n",
       "├─MaxPool2d: 1-17                        [10, 256, 4, 4]           --\n",
       "├─Linear: 1-18                           [10, 2048]                8,390,656\n",
       "│    └─weight                                                      ├─8,388,608\n",
       "│    └─bias                                                        └─2,048\n",
       "├─Linear: 1-19                           [10, 512]                 1,049,088\n",
       "│    └─weight                                                      ├─1,048,576\n",
       "│    └─bias                                                        └─512\n",
       "├─Linear: 1-20                           [10, 128]                 65,664\n",
       "│    └─weight                                                      ├─65,536\n",
       "│    └─bias                                                        └─128\n",
       "==========================================================================================\n",
       "Total params: 12,675,392\n",
       "Trainable params: 12,675,392\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 3.82\n",
       "==========================================================================================\n",
       "Input size (MB): 1.51\n",
       "Forward/backward pass size (MB): 47.88\n",
       "Params size (MB): 50.70\n",
       "Estimated Total Size (MB): 100.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print summary\n",
    "model = Facenet_NN1()\n",
    "summary(model, input_size=(10, 3, 112, 112), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fcaa6b-902b-4f5c-b48b-2ec1aa46ee89",
   "metadata": {},
   "source": [
    "# Train Facenet Model\n",
    "training data\n",
    "In all our experiments we train the CNN using Stochastic\n",
    "Gradient Descent (SGD) with standard backprop [8, 11] and\n",
    "AdaGrad [5]. In most experiments we start with a learning\n",
    "rate of 0.05 which we lower to finalize the model. The models are initialized from random, similar to [16], and trained\n",
    "on a CPU cluster for 1,000 to 2,000 hours. The decrease in\n",
    "the loss (and increase in accuracy) slows down drastically\n",
    "after 500h of training, but additional training can still significantly improve performance. The margin α is set to 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df9202-813b-4d3e-8b5a-d4f5e6167eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# online triplet selection\n",
    "# https://github.com/adambielski/siamese-triplet\n",
    "# https://github.com/adambielski/siamese-triplet/blob/master/utils.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc018e-1f6d-4840-ab6f-f2bb505cbdc0",
   "metadata": {},
   "source": [
    "# Test Facenet Model\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694dafc5-06c2-486c-997b-b4f97ac4f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
